{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration de Cuda dans Google Colab"
      ],
      "metadata": {
        "id": "qE-HbkLWqjJK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJjU5zI_-tjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2fc50a-91af-4c03-8693-28a82c85fbc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "id": "HBZjh4P4-1HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7782109-c0aa-40f2-b406-a8620f40edc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-zfvcnosn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-zfvcnosn\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4305 sha256=33dca181e7d2cc9d645eec7fb2479f60637aa109b7be115988888bf84e2f95fe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gudyi3kd/wheels/db/c1/1f/a2bb07bbb4a1ce3c43921252aeafaa6205f08637e292496f04\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On vérifie que l'on est bien connecté au GPU"
      ],
      "metadata": {
        "id": "YJw3IWdqtpfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "9gl_4Pn7_JR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b91602f-4dd9-439f-a70a-9f99b185434e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 24 12:43:00 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du plugin nvcc permettant de compiler/executer les programmes Cuda"
      ],
      "metadata": {
        "id": "NlBBvVVOt4Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "id": "6YGSePh_Q_DP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6365079d-12f2-4729-a17b-b215bd94987b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Un makefile est déjà à votre disposition pour compiler les programme du TP\n"
      ],
      "metadata": {
        "id": "0zrHyBuVuQpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Executez la cellule du Makefile\n",
        "\n",
        "Le makefile a été modifié pour les programmes puisse s'exécuter avec la GPU premium.\n",
        "\n"
      ],
      "metadata": {
        "id": "AWS3uSGwu2o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Makefile\n",
        "# Change the example variable to build a different source module (e.g. EXAMPLE=exercice01)\n",
        "EXAMPLE=program\n",
        "\n",
        "# Makefile variables \n",
        "# Add extra targets to OBJ with space separator e.g. If there is as source file random.c then add random.o to OBJ)\n",
        "# Add any additional dependancies (header files) to DEPS. e.g. if there is aheader file random.h required by your source modules then add this to DEPS.\n",
        "CC=gcc\n",
        "CFLAGS= -O3 -Wextra -fopenmp\n",
        "NVCC=nvcc\n",
        "NVCC_FLAGS = -DUSE_STREAMS -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87\n",
        "OBJ=$(EXAMPLE).o\n",
        "DEPS=\n",
        "\n",
        "# Build rule for object files ($@ is left hand side of rule, $< is first item from the right hand side of rule)\n",
        "%.o : %.cu $(DEPS)\n",
        "\t$(NVCC) -c -o $@ $< $(NVCC_FLAGS) $(addprefix -Xcompiler ,$(CCFLAGS))\n",
        "\n",
        "# Make example ($^ is all items from right hand side of the rule)\n",
        "$(EXAMPLE) : $(OBJ)\n",
        "\t$(NVCC) -o $@ $^ $(NVCC_FLAGS) $(addprefix -Xcompiler ,$(CCFLAGS))\n",
        "\n",
        "# PHONY prevents make from doing something with a filename called clean\n",
        ".PHONY : clean\n",
        "clean:\n",
        "\trm -rf $(EXAMPLE) $(OBJ)"
      ],
      "metadata": {
        "id": "2VMs5wjdRU0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558af990-d85f-40a9-8b5e-71571f386e7f"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP4\n"
      ],
      "metadata": {
        "id": "ri3OU-i0vt2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.h\n",
        "#ifndef __UTILS_H__\n",
        "#define __UTILS_H__\n",
        "#include <stdio.h>\n",
        "\n",
        "static void HandleError( cudaError_t err,\n",
        "                         const char *file,\n",
        "                         int line ) {\n",
        "    if (err != cudaSuccess) {\n",
        "        printf( \"%s in %s at line %d\\n\", cudaGetErrorString( err ),\n",
        "                file, line );\n",
        "        exit( EXIT_FAILURE );\n",
        "    }\n",
        "}\n",
        "#define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))\n",
        "\n",
        "\n",
        "#define HANDLE_NULL( a ) {if (a == NULL) { \\\n",
        "                            printf( \"Host memory failed in %s at line %d\\n\", \\\n",
        "                                    __FILE__, __LINE__ ); \\\n",
        "                            exit( EXIT_FAILURE );}}\n",
        "\n",
        "template< typename T >\n",
        "void swap( T& a, T& b ) {\n",
        "    T t = a;\n",
        "    a = b;\n",
        "    b = t;\n",
        "}\n",
        "\n",
        "\n",
        "void* big_random_block( int size ) {\n",
        "    unsigned char *data = (unsigned char*)malloc( size );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "int* big_random_block_int( int size ) {\n",
        "    int *data = (int*)malloc( size * sizeof(int) );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "\n",
        "// a place for common kernels - starts here\n",
        "\n",
        "__device__ unsigned char value( float n1, float n2, int hue ) {\n",
        "    if (hue > 360)      hue -= 360;\n",
        "    else if (hue < 0)   hue += 360;\n",
        "\n",
        "    if (hue < 60)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*hue/60));\n",
        "    if (hue < 180)\n",
        "        return (unsigned char)(255 * n2);\n",
        "    if (hue < 240)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*(240-hue)/60));\n",
        "    return (unsigned char)(255 * n1);\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( unsigned char *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset*4 + 0] = value( m1, m2, h+120 );\n",
        "    optr[offset*4 + 1] = value( m1, m2, h );\n",
        "    optr[offset*4 + 2] = value( m1, m2, h -120 );\n",
        "    optr[offset*4 + 3] = 255;\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( uchar4 *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset].x = value( m1, m2, h+120 );\n",
        "    optr[offset].y = value( m1, m2, h );\n",
        "    optr[offset].z = value( m1, m2, h -120 );\n",
        "    optr[offset].w = 255;\n",
        "}\n",
        "\n",
        "\n",
        "#if _WIN32\n",
        "    //Windows threads.\n",
        "    #include <windows.h>\n",
        "\n",
        "    typedef HANDLE CUTThread;\n",
        "    typedef unsigned (WINAPI *CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC unsigned WINAPI\n",
        "    #define  CUT_THREADEND return 0\n",
        "\n",
        "#else\n",
        "    //POSIX threads.\n",
        "    #include <pthread.h>\n",
        "\n",
        "    typedef pthread_t CUTThread;\n",
        "    typedef void *(*CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC void\n",
        "    #define  CUT_THREADEND\n",
        "#endif\n",
        "\n",
        "//Create thread.\n",
        "CUTThread start_thread( CUT_THREADROUTINE, void *data );\n",
        "\n",
        "//Wait for thread to finish.\n",
        "void end_thread( CUTThread thread );\n",
        "\n",
        "//Destroy thread.\n",
        "void destroy_thread( CUTThread thread );\n",
        "\n",
        "//Wait for multiple threads.\n",
        "void wait_for_threads( const CUTThread *threads, int num );\n",
        "\n",
        "#if _WIN32\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void *data){\n",
        "        return CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE)func, data, 0, NULL);\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        WaitForSingleObject(thread, INFINITE);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        TerminateThread(thread, 0);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        WaitForMultipleObjects(num, threads, true, INFINITE);\n",
        "\n",
        "        for(int i = 0; i < num; i++)\n",
        "            CloseHandle(threads[i]);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void * data){\n",
        "        pthread_t thread;\n",
        "        pthread_create(&thread, NULL, func, data);\n",
        "        return thread;\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        pthread_join(thread, NULL);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        pthread_cancel(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        for(int i = 0; i < num; i++)\n",
        "            end_thread( threads[i] );\n",
        "    }\n",
        "\n",
        "#endif\n",
        "\n",
        "#endif  // __UTILS_H__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8DiZxqTVzyD",
        "outputId": "e7624f06-32ad-4969-f50e-d5814f500956"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mémoire paginée vs. Mémoire épinglée.\n",
        "\n",
        "Afin de visualiser le gain que l'on obtient en utilisant la mémoire épinglée, vous allez implémentez deux fonctions **cuda_malloc(int n, bool up)** et **cuda_host_alloc(int n, bool up)**\n",
        "\n",
        "\n",
        "\n",
        "### cuda_malloc() \n",
        "#### 1.1 Allouez la mémoire hôte pour le vecteur a à l'aide de malloc\n",
        "#### 1.2 Allouez la mémoire GPU pour le vecteur dev_A à l'aide de cudaMalloc\n",
        "#### 1.3 Ecrire une boucle pour i allant de 0 à 100 :\n",
        "  - si up == True alors copiez le vecteur a dans dev_a\n",
        "  - sinon copiez le vecteur dev_a dans a.\n",
        "\n",
        "#### 1.4 Libérez la mémoire.\n",
        "\n",
        "### cuda_host_alloc()\n",
        "#### 1.4 Allouez la mémoire hôte pour le vecteur a à l'aide de malloc\n",
        "#### 1.5 Allouez la mémoire GPU pour le vecteur dev_A à l'aide de cudaMalloc\n",
        "#### 1.6 Ecrire une boucle pour i allant de 0 à 100 :\n",
        "  - si up == True alors copiez le vecteur a dans dev_a\n",
        "  - sinon copiez le vecteur dev_a dans a.\n",
        "\n",
        "#### 1.7 Libérez la mémoire.\n",
        "\n",
        "Constatez-vous une différence entre la mémoire paginée et la mémoire épinglée ?\n",
        "\n",
        "OUi, il y a une grande difference entre la memoire paginé et la memoire epinglé comme le montre les données qui sont données en bas.\n",
        "Le débit est beacoup plus faible avec la mémoire paginé. Le transfert sera donc plus rapide entre le CPU et le GPU quand on utilise la memoire épinglé.\n"
      ],
      "metadata": {
        "id": "uB1BFcg5Qxb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program.cu\n",
        "#include \"utils.h\"\n",
        "\n",
        "#define N  (64*1024*1024)\n",
        "\n",
        "\n",
        "float cuda_malloc_test( int n, bool up ) {\n",
        "    cudaEvent_t     start, stop;\n",
        "    int             *a, *d_a;\n",
        "    float           elapsedTime;\n",
        "    unsigned int size = n * sizeof(int);\n",
        "    cudaStream_t stream;\n",
        "\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    // 1.1 Allouez de la mémoire hôte pour le vecteur a de taille n (malloc)\n",
        "    a = (int*) malloc(size);\n",
        "    // 1.2 Allouez de la mémoire GPU pour dev_a de taille n\n",
        "    HANDLE_ERROR(cudaMalloc(&d_a, size));\n",
        "\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "    // 1.3 Pour i allant de 0 à 100 : si up == True alors copiez le vecteur a dans dev_a, sinon copiez le vecteur dev_a dans a.\n",
        "    for(int i=0; i<100 ; i++){\n",
        "      if(up) {cudaMemcpyAsync(d_a, a, size, cudaMemcpyHostToDevice, stream);}\n",
        "      else {cudaMemcpyAsync(a, d_a, size, cudaMemcpyDeviceToHost, stream);}\n",
        "    }\n",
        "    \n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,\n",
        "                                        start, stop ) );\n",
        "\n",
        "    // 1.4 Libérez la mémoire.\n",
        "    HANDLE_ERROR( cudaEventDestroy( start ) );\n",
        "    // Voir si ya pas un pb ici\n",
        "    free(a);\n",
        "    cudaFree(d_a);\n",
        "    HANDLE_ERROR( cudaEventDestroy( stop ) );\n",
        "    cudaStreamDestroy(stream);\n",
        "    return elapsedTime;\n",
        "}\n",
        "\n",
        "\n",
        "float cuda_host_alloc_test( int n, bool up ) {\n",
        "    cudaEvent_t     start, stop;\n",
        "    int             *a, *d_a;\n",
        "    float           elapsedTime;\n",
        "    unsigned int size    =       n * sizeof(int);\n",
        "\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "\n",
        "    // 1.5 Allouez de la mémoire hôte dans la mémoire épinglée pour le vecteur a de taille n (cudaHostAlloc)\n",
        "\n",
        "    HANDLE_ERROR(cudaMallocHost(&a, size));\n",
        "\n",
        "    // 1.6 Allouez de la mémoire GPU pour dev_a de taille n\n",
        "\n",
        "    HANDLE_ERROR(cudaMalloc((void**)&d_a, size));\n",
        "\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "    \n",
        "    // 1.7 Pour i allant de 0 à 100 : si up == True alors copiez le vecteur a dans dev_a, sinon copiez le vecteur dev_a dans a.\n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    for(int i=0; i<100 ; i++){\n",
        "      if(up) {cudaMemcpyAsync(d_a, a, size, cudaMemcpyHostToDevice);}\n",
        "      else {cudaMemcpyAsync(a, d_a, size, cudaMemcpyDeviceToHost);}\n",
        "    }\n",
        "\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,\n",
        "                                        start, stop ) );\n",
        "\n",
        "    // 1.8 Libérez la mémoire.\n",
        "    HANDLE_ERROR( cudaEventDestroy( start ) );\n",
        "    HANDLE_ERROR( cudaEventDestroy( stop ) );\n",
        "\n",
        "    cudaFreeHost(a);\n",
        "    cudaFree(d_a);\n",
        "\n",
        "\n",
        "    return elapsedTime;\n",
        "}\n",
        "\n",
        "\n",
        "int main( void ) {\n",
        "    float           elapsedTime;\n",
        "    float           MB = (float)100*N*sizeof(int)/1024/1024;\n",
        "\n",
        "\n",
        "    // try it with cudaMalloc\n",
        "    elapsedTime = cuda_malloc_test( N, true );\n",
        "    printf( \"Time using cudaMalloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy up:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "\n",
        "    elapsedTime = cuda_malloc_test( N, false );\n",
        "    printf( \"Time using cudaMalloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy down:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "\n",
        "    // now try it with cudaHostAlloc\n",
        "    elapsedTime = cuda_host_alloc_test( N, true );\n",
        "    printf( \"Time using cudaHostAlloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy up:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "\n",
        "    elapsedTime = cuda_host_alloc_test( N, false );\n",
        "    printf( \"Time using cudaHostAlloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy down:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "}"
      ],
      "metadata": {
        "id": "Rm-7gDO8BGH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5343daa5-a987-4e73-cc87-7e492bc395d0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting program.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "id": "PfakPQ1CHLUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339e92d7-cba2-4a9c-e157-73ce6a18ad73"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc -c -o program.o program.cu -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87 \n",
            "nvcc -o program program.o -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./program"
      ],
      "metadata": {
        "id": "F4BRhsCtHLo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc953fa7-28e1-4974-8d37-eb4b0742eadb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time using cudaMalloc:  4072.5 ms\n",
            "\tMB/s during copy up:  6286.1\n",
            "Time using cudaMalloc:  5760.5 ms\n",
            "\tMB/s during copy down:  4444.0\n",
            "Time using cudaHostAlloc:  0.0 ms\n",
            "\tMB/s during copy up:  7339449856.0\n",
            "Time using cudaHostAlloc:  0.0 ms\n",
            "\tMB/s during copy down:  8333333504.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Streams et execution asynchrone\n",
        "\n",
        "Afin de visualiser le gain que l'on obtient en utilisant la mémoire épinglée, vous allez implémentez deux fonctions **cuda_malloc(int n, bool up)** et **cuda_host_alloc(int n, bool up)**\n",
        "\n",
        "\n",
        "\n",
        "### Mémoire paginée vs. épinglée\n",
        "  Pour le moment la mémoire pour les vecteurs host h_x, h_y et h_y1 est alloué via la fonction malloc. La mémoire alloué par malloc est paginable.\n",
        "  \n",
        "#### 2.1 Compilez , executez le programme et relevez le temps d'execution en sortie du programme.\n",
        "\n",
        "**non-stream elapsed time: 0.257619**\n",
        "\n",
        "#### 2.2 Modifiez le code de sorte à n'utiliser que de la mémoire épingler via cudaHostMalloc(). \n",
        "N'oubliez pas de changer la façon dont on libère la mémoire en fin de programme.\n",
        "\n",
        "Executez à nouveau le programme, que constatez-vous sur le temps d'exécution ?\n",
        "\n",
        "**non-stream elapsed time: 0.071484  \n",
        "Il est beaucoup plus rapide en raison que le cudaHostMalloc permet quer la mémoire épinglé soit directement accessible par le CPU.**\n",
        "\n",
        "### Streams\n",
        "Vous allez maintenant modifier le programme afin de rendre possible l'execution asynchrone entre les copies CPU <--> GPU et les kernels.\n",
        "\n",
        "Le but etant de diviser le vecteurs 1D h_x en un sous_ensemble de vecteurs definit par la variable constante subpart. Un stream s'occupera d'un sous-vecteur à la fois.\n",
        "\n",
        "```c\n",
        "typedef float ft;\n",
        "const int sub_parts = 64;\n",
        "const size_t ds = 1024*1024*sub_parts;\n",
        "const int count = 22;\n",
        "const int num_streams = 8;\n",
        "```\n",
        "\n",
        "Dans le code, differentes constantes ont été déclarés:\n",
        "\n",
        "- **ft** : type utilisé pour declarer nos variables dans le code (changer float par double pour utiliser la précision double)\n",
        "- **sub_paths** : Permet de diviser notre vecteur en un sous-ensemble de vecteurs.\n",
        "- **ds** : Correspond à la taille totale du vecteur. Dans la partie streams du TP, sub_parts permet d'avoir 64 vecteurs de taille 1024*1024 \n",
        "- **count** : Permet de définir la taille de l'interval de valeurs utilisé pour calculer la moyenne de la probabilité de densité.\n",
        "- **num_streams** : Correspond au nombre de streams que l'on veut lancer.\n",
        "\n",
        "Vous ecrirez le code correspondant aux streams dans la partie délimité par **#ifdef USE_STREAM #endif**\n",
        "\n",
        "```c\n",
        "#ifdef USE_STREAMS\n",
        "\n",
        "\t// Code correspondant aux streams\n",
        "\n",
        "#endif\n",
        "```\n",
        "\n",
        "#### 2.3 Creation des streams\n",
        "\n",
        "Ecrivez le ou les instructions code permettant de creer vos streams de tailles num_streams en utilisant la fonction **cudaStreamCreate()**\n",
        "\n",
        "#### 2.4 Destruction des streams\n",
        "\n",
        "Ecrivez le ou les instructions permettant de détruire vos streams en utilisant la fonction **cudaStreamDestroy()**.\n",
        "\n",
        "#### 2.5 Execution des streams\n",
        "\n",
        "Pour chaque stream : \n",
        "\n",
        "- Faite une copie asynchrone CPU vers GPU du vecteur **h_x** dans **d_x** via **cudaMemcpyAsync()**.\n",
        "- Lancer le kernel pour le stream courant\n",
        "- Faite la copie GPU vers CPU du résultat **d_y** dans **h_y** via **cudaMemcpyAsync()**.\n",
        "\n",
        "Pour rappel, chaque streams s'occupe d'un sous-ensemble du vecteur **h_x**. \n",
        "\n",
        "Si **h_x** est divisé en 64 sous-vecteurs et nous n'avons que 8 streams alors les treams 0, 1, 2, ..., 7 s'occuperont respectivement des sous-vecteurs 0, 1, 2, ... 7 puis des sous-vecteurs 8, 9, 10, ..., 15 et ainsi de suite... \n",
        "Pour les streams pensez à utiliser un offset, pour retourver l'id du stream vous pouvez utiliser le module % :\n",
        "\n",
        "- 0%4 = 0, 1%4 = 1, 2%4 = 2, 3%4 = 3\n",
        "- 4%4 = 0, 5%4 = 1, 6%4 = 2, 7%4 = 3\n",
        "\n",
        "Chaque sous-vecteur est de taille 1024x1024, donc : \n",
        "\n",
        "- Le stream 0 s'occupera du sous-vecteur 0 de taille 1024x1024 commençant par l'indice 0.\n",
        "- Le stream 1 s'occupera du sous-vecteur 1 de taille 1024x1024 commençant par l'indice (1024x1024).\n",
        "- Le stream 2 s'occupera du sous-vecteur 2 de taille 1024x1024 commençant par l'indice 2*(1024x1024).\n",
        "\n",
        "Si vous exécuté le code, une étape de vérification sera effectué pour s'assurer que votre implémentation est correcte.\n",
        "\n",
        "#### 2.6 Execution asynchrone\n",
        "\n",
        "Pour executer le code en mode streams, c'est à dire la partie délimité par **#ifdef USE_STREAM #endif**, rajouter le flag -DUSE_STREAMS dans le makefile.\n",
        "\n",
        "```c\n",
        "NVCC_FLAGS = -DUSE_STREAMS -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87\n",
        "```\n",
        "Executez le code et comparez les temps d'execution non-streams vs. streams.\n",
        "\n",
        "Que constatez-vous ?\n",
        "\n",
        "#### 2.7 Modifiez les paramètres\n",
        "```c\n",
        "typedef float ft;\n",
        "const int sub_parts = 64;\n",
        "const size_t ds = 1024*1024*sub_parts;\n",
        "const int count = 22;\n",
        "const int num_streams = 8;\n",
        "```\n",
        "Que se passe t'il au niveau du temps d'execution lorsque vous :\n",
        "\n",
        "1. Changez **typedef float ft** par **typedef double ft** ?\n",
        "2. Augmentez ou diminuez de manière considerable **sub_parts** ?\n",
        "3. Changez la taille **ds** ?\n",
        "3. Augmentez ou diminuez **count** ? \n",
        "4. Augmentez ou diminuez **num_streams** ? (nombre limité par GPU)\n",
        "\n"
      ],
      "metadata": {
        "id": "hrMijrhjbjJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program.cu\n",
        "#include <math.h>\n",
        "#include <iostream>\n",
        "#include <time.h>\n",
        "#include <sys/time.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "typedef float ft;\n",
        "const int sub_parts = 64;\n",
        "const size_t ds = 1024*1024*sub_parts;\n",
        "const int count = 22;\n",
        "const int num_streams = 8;\n",
        "\n",
        "const float sqrt_2PIf = 2.5066282747946493232942230134974f;\n",
        "const double sqrt_2PI = 2.5066282747946493232942230134974;\n",
        "__device__ float gpdf(float val, float sigma) {\n",
        "  return expf(-0.5f * val * val) / (sigma * sqrt_2PIf);\n",
        "}\n",
        "\n",
        "__device__ double gpdf(double val, double sigma) {\n",
        "  return exp(-0.5 * val * val) / (sigma * sqrt_2PI);\n",
        "}\n",
        "\n",
        "//  calcul la moyenne de la densite de probabilite sur un interval de valeurs autour de chaque point.\n",
        "__global__ void gaussian_pdf(const ft * __restrict__ x, ft * __restrict__ y, const ft mean, const ft sigma, const int n) {\n",
        "  int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if (idx < n) {\n",
        "    ft in = x[idx] - (count / 2) * 0.01f;\n",
        "    ft out = 0;\n",
        "    for (int i = 0; i < count; i++) {\n",
        "      ft temp = (in - mean) / sigma;\n",
        "      out += gpdf(temp, sigma);\n",
        "      in += 0.01f;\n",
        "    }\n",
        "    y[idx] = out / count;\n",
        "  }\n",
        "}\n",
        "\n",
        "// Verification d'erreur CUDA\n",
        "#define cudaCheckErrors(msg) \\\n",
        "  do { \\\n",
        "    cudaError_t __err = cudaGetLastError(); \\\n",
        "    if (__err != cudaSuccess) { \\\n",
        "        fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
        "            msg, cudaGetErrorString(__err), \\\n",
        "            __FILE__, __LINE__); \\\n",
        "        fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
        "        exit(1); \\\n",
        "    } \\\n",
        "  } while (0)\n",
        "\n",
        "// Calcul du temps sur l'host\n",
        "#define USECPSEC 1000000ULL\n",
        "\n",
        "unsigned long long dtime_usec(unsigned long long start) {\n",
        "  timeval tv;\n",
        "  gettimeofday(&tv, 0);\n",
        "  return ((tv.tv_sec*USECPSEC)+tv.tv_usec)-start;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  ft *h_x, *d_x, *h_y, *h_y1, *d_y;\n",
        "  //h_x = (ft *)malloc(ds*sizeof(ft));\n",
        "  //h_y = (ft *)malloc(ds*sizeof(ft));\n",
        "  //h_y1 = (ft *)malloc(ds*sizeof(ft));\n",
        "\n",
        "  cudaMallocHost(&h_x, ds*sizeof(ft));\n",
        "  cudaMallocHost(&h_y, ds*sizeof(ft));\n",
        "  cudaMallocHost(&h_y1, ds*sizeof(ft));\n",
        "\n",
        "  cudaMalloc(&d_x, ds*sizeof(ft));\n",
        "  cudaMalloc(&d_y, ds*sizeof(ft));\n",
        "  cudaCheckErrors(\"allocation error\");\n",
        "\n",
        "  gaussian_pdf<<<(ds + 255) / 256, 256>>>(d_x, d_y, 0.0, 1.0, ds); // warm-up\n",
        "\n",
        "  for (size_t i = 0; i < ds; i++) {\n",
        "    h_x[i] = rand() / (ft)RAND_MAX;\n",
        "  }\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  unsigned long long et1 = dtime_usec(0);\n",
        "\n",
        "  cudaMemcpy(d_x, h_x, ds * sizeof(ft), cudaMemcpyHostToDevice);\n",
        "  gaussian_pdf<<<(ds + 255) / 256, 256>>>(d_x, d_y, 0.0, 1.0, ds);\n",
        "  cudaMemcpy(h_y1, d_y, ds * sizeof(ft), cudaMemcpyDeviceToHost);\n",
        "  cudaCheckErrors(\"non-streams execution error\");\n",
        "\n",
        "  et1 = dtime_usec(et1);\n",
        "  std::cout << \"non-stream elapsed time: \" << et1/(float)USECPSEC << std::endl;\n",
        "\n",
        "#ifdef USE_STREAMS\n",
        "  cudaMemset(d_y, 0, ds * sizeof(ft));\n",
        "\n",
        "  unsigned long long et = dtime_usec(0);\n",
        "\n",
        "  // 2.3 Creation des streams\n",
        "  cudaStream_t stream[num_streams];\n",
        "  for (int i=0; i< num_streams; i++){\n",
        "    cudaStreamCreate(&stream[i]);\n",
        "  }\n",
        "\n",
        "  // 2.5 Execution des streams\n",
        "  \n",
        "  const int streamSize  = ds;\n",
        "\n",
        "  int blockSize = sub_parts;  \n",
        "\n",
        "  for (int i=0; i < num_streams; i++){\n",
        "    int offset = i % 4;\n",
        "    cudaMemcpyAsync(&d_x[offset], &h_x[offset], streamSize * sizeof(ft), cudaMemcpyHostToDevice, stream[i]);\n",
        "    gaussian_pdf<<< (streamSize / blockSize), blockSize, 0, stream[i]>>>(d_x, d_y, 0.0, 1.0, ds/8);\n",
        "    cudaMemcpyAsync(&h_y[offset], &d_y[offset], streamSize * sizeof(ft), cudaMemcpyDeviceToHost, stream[i]);\n",
        "    \n",
        "  }\n",
        " \n",
        "\n",
        "\n",
        "  //cudaDeviceSynchronize();\n",
        "  for(int i= 0; i< num_streams; i++){\n",
        "    cudaStreamSynchronize(stream[i]);\n",
        "  }\n",
        "\n",
        "   et = dtime_usec(et);\n",
        "\n",
        "\n",
        "  for (int i = 0; i < ds; i++) {\n",
        "    if (h_y[i] != h_y1[i]) {\n",
        "      std::cout << \"mismatch at \" << i << \" was: \" << h_y[i] << \" should be: \" << h_y1[i] << std::endl;\n",
        "      return -1;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  std::cout << \"streams elapsed time: \" << et/(float)USECPSEC << std::endl;\n",
        "  // 2.4 Destruction des streams\n",
        "\n",
        "  for (int i=0; i< num_streams; i++){\n",
        "    cudaStreamDestroy(stream[i]);\n",
        "  }\n",
        "  \n",
        "\n",
        "  \n",
        "#endif\n",
        "\n",
        "    cudaFreeHost(h_y1);\n",
        "    cudaFreeHost(h_y);\n",
        "    cudaFreeHost(h_x);\n",
        "\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "QKhzrIKZiMcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2f5b28-411c-4bfe-f0ac-1365b1f86d5a"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting program.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnF_eWyL4LtV",
        "outputId": "99254ab5-5cfc-4c86-da93-1947eff43254"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make: 'program' is up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./program"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vjuu-ha4Oom",
        "outputId": "91387343-8c50-4787-fb31-99352d1e4e97"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non-stream elapsed time: 0.07012\n",
            "mismatch at 8388608 was: 0 should be: 0.301959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ItltqFy0zM3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}